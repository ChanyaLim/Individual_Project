---
title: "Mushroom Capstone Project"
author: "Chanya Limdamnern"
date: "22/5/2564"
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage
# Introduction
This capstone project is the second part of PH125.9x: Data Science: Capstone course. For this project, mushroom class(edible or poisonous) prediction model will be developed using mushrooms dataset downloaded from Kaggle and kept in my GitHub.

The goal of this project is to develop model for mushroom class prediction which maximize poisonous mushroom prediction accuracy.

## Overview
The first step of coding is to install and load required package which is not shown the code here. Next step is to import mushroom dataset from my GitHub into R.

```{r package installation, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
if(!require(xgboost))install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org")
```

```{r package loading, include=FALSE}
library(tidyverse)
library(caret)
library(readr)
library(data.table)
library(dplyr)
library(randomForest)
library(e1071)
library(xgboost)
library(tinytex)
```

```{r mushroom.csv import}
### import mushroom dataset from my github 
url<- "https://raw.githubusercontent.com/ChanyaLim/Individual_Project/master/mushrooms.csv"
## assign column names 
col_name <- c("class","cap_shape","cap_surface","cap_color",
              "bruises","odor","gill_attachment","gill_spacing",
              "gill_size","gill_color","stalk_shape","stalk_root",
              "stalk_surface_above_ring","stalk_surface_below_ring",
              "stalk_color_above_ring","stalk_color_below_ring",
              "veil_type","veil_color","ring_number","ring_type","spore_print_color",
              "population","habitat")
## import mushrooms.csv file 
mushroom <- fread(url,col.names=col_name)
## change character data to factor 
mushroom <- mushroom %>% mutate_if(is.character, as.factor)
```

Data structure below shows that mushroom dataset have 23 variables and 8124 observations. You can see that all variables are factor(categorical data). Meaning of levels of each variable is in the Appendix A.

```{r mushroom}
str(mushroom,width=80,strict.width="cut")
```

## Executive summary
Now mushroom dataset is imported into R already. Next step is data cleaning including NA, duplicate, unknown data and non-significant data such as single level factor variable validation.

After data cleaning finished, data exploration and visualization will be introduced. This section is to select significant variable with mushroom class for model development.

Once we get the variables for our model development, next step is to split mushroom dataset to be train set for model development and test set for model validation by using confusion matrix. The model which maximize poisonous mushroom prediction accuracy will be the final model for mushroom class prediction.

# Methods and analysis
## Data cleaning
### NA 
The result from code below shows `r any(is.na(mushroom))` means there is no NA in our mushroom dataset.

```{r NA}
any(is.na(mushroom))
```

### Duplicated data 

The result from code below shows `r sum(duplicated(mushroom))` means there is no duplicated observation in our mushroom dataset.

```{r Duplicated}
sum(duplicated(mushroom))
```

### Unknown and non significant data

Below dataframe shows summary of level number and levels for each variable. From this summary, you will see that there is "?" level in stalk root variable which means unknown data and there is only 1 level in veil type which is non-significant for model development so I decided to remove these two variables from the dataset.

```{r factor summary}
level <- sapply(mushroom,levels)
Number_level <- sapply(mushroom,uniqueN)
dataoverview <- as.data.frame(Number_level) %>% mutate(Level=level)
dataoverview
```

```{r remove variable}
mushroom <- mushroom %>% select(-"veil_type")
mushroom <- mushroom %>% select(-"stalk_root")
```

Now data cleaning step have been finished. Our mushroom dataset have no NA data, no duplicated observation and already removed variables which is non-significant and have unknown data.

\newpage
## Data exploration and visualization
### Class
This variable is the target prediction including 2 levels; e is edible and p is poisonous.
Graph below shows proportion of each level in class variable. The proportion of e : p is 51.8 : 48.2 so this data is balance.

```{r class variable,fig.align = 'center',out.width="70%",echo=FALSE}
mushroom %>% group_by(class) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(class,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))+
  geom_text(aes(label=round(Percent*100,digit=2)),vjust=-0.25)
```

Next step is to explore each variable and select variable which will be used in model development step.
For variable selection, proportion of each variable and proportion compare to class will be plotted.
 
First group of selected variables are variable which each levels proportion is more than 1% and proportion compare to class variable is significant.
The reason to select only more than 1% each level is to minimize overfitting problem in the prediction model. 

Second group of selected variables will consider others variable further from first group and add variable which proportion compare to class variable of big portion level is significant. Due to this group consider less than 1% each level, it may lead to overfitting problem so during model development, we need to check variable important and see if less portion level is top important. Then, adjust the model to minimize overfitting.

\newpage
### Cap shape
The first graph below shows level c and s are less than 1% and big portion are level f and x.

The second graph shows big portion levels are not significant to class because proportion between poisonous and edible of level f and x are not much different so this variable is not used in model development step.

```{r cap_shape variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(cap_shape) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(cap_shape,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(cap_shape,fill=class))+
  geom_bar(position = "dodge")
```

### Cap surface
The first graph below shows level g is less than 1% and big portion are f,s and y level.

The second graph shows f level is significant to class because large majority of f is edible so this variable will be in second group.

```{r cap_surface variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(cap_surface) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(cap_surface,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(cap_surface,fill=class))+
  geom_bar(position = "dodge")
```

\newpage
### Cap color
The first graph below shows level c, r and u are less than 1% and big portion are e, g, n, w and y level.

The second graph shows e, g, n, w and y level are significant to class because large majority of w is edible, majority of e, y are poisonous and majority of g, n are edible so this variable will be in second group.

```{r cap_color variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(cap_color) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(cap_color,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(cap_color,fill=class))+
  geom_bar(position = "dodge")
```

### Bruises
The first graph below shows no level is less than 1% and both f and t level are big portion.

The second graph shows f and t level are significant to class because large majority of f is poisonous and large majority of t is edible so this variable will be in first group.

```{r bruises variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(bruises) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(bruises,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(bruises,fill=class))+
  geom_bar(position = "dodge")
```

\newpage
### Odor
The first graph below shows level m is less than 1% and big portion are f and n level.

The second graph shows both f and n level are significant to class because all f is poisonous and large majority of n is edible so this variable will be in second group.

```{r odor variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(odor) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(odor,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(odor,fill=class))+
  geom_bar(position = "dodge")
```

### Gill attachment
The first graph below shows level a is less than 1% and big portion is level f.

The second graph shows level f is not significant to class so this variable is not used in model development step.

```{r gill_attachment variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(gill_attachment) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(gill_attachment,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(gill_attachment,fill=class))+
  geom_bar(position = "dodge")
```

\newpage
### Gill spacing
The first graph below shows no level is less than 1% and c level is big portion.

The second graph shows c level is not quite significant to class even though majority of c is poisonous but proportion between poisonous and edible is not much different. And you can see w level which is small portion but large majority of w is edible. From this exploration, this variable will also be in first group.

```{r gill_spacing variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(gill_spacing) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(gill_spacing,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(gill_spacing,fill=class))+
  geom_bar(position = "dodge")
```

### Gill size
The first graph below shows no level is less than 1% and both b and n level are big portion.

The second graph shows b and n level are significant to class because large majority of b is edible and large majority of n is poisonous so this variable will be in first group.

```{r gill_size variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(gill_size) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(gill_size,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(gill_size,fill=class))+
  geom_bar(position = "dodge")
```

\newpage
### Gill color
The first graph below shows level r is less than 1% and big portion are b, g, h, n, p and w level.

The second graph shows b, h, n and w level are significant to class because all b is poisonous, large majority of h is poisonous and large majority of n and w are edible so this variable will be in second group.

```{r gill_color variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(gill_color) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(gill_color,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(gill_color,fill=class))+
  geom_bar(position = "dodge")
```

### Stalk shape
The first graph below shows no level is less than 1% and both e and t level are big portion.

The second graph shows b and n level are not quite significant to class even though majority of e is poisonous and majority of t is edible but proportion between poisonous and edible is not much different. From this exploration, this variable will also be in first group and we can see in variable importance of the model if this variable is significant.

```{r stalk shape variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(stalk_shape) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(stalk_shape,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(stalk_shape,fill=class))+
  geom_bar(position = "dodge")
```

\newpage
### Stalk surface above ring
The first graph below shows level y is less than 1% and big portion are k and s level.

The second graph shows k and s level are significant to class because large majority of k is poisonous and large majority of s is edible so this variable will be in second group.
```{r stalk surface above ring variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(stalk_surface_above_ring) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(stalk_surface_above_ring,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(stalk_surface_above_ring,fill=class))+
  geom_bar(position = "dodge")

```

### Stalk surface below ring
This variable characteristic is similar to stalk surface above ring so this variable will be in second group.
```{r stalk surface below ring variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(stalk_surface_below_ring) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(stalk_surface_below_ring,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(stalk_surface_below_ring,fill=class))+
  geom_bar(position = "dodge")

```

\newpage
### Stalk color above ring
The first graph below shows level c and y are less than 1% and big portion are p and w level.

The second graph shows p and w level are significant to class because large majority of p is poisonous and majority of w is edible so this variable will be in second group.
```{r stalk color above ring variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(stalk_color_above_ring) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(stalk_color_above_ring,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(stalk_color_above_ring,fill=class))+
  geom_bar(position = "dodge")
```

### Stalk color below ring
This variable characteristic is similar to stalk color above ring so this variable will be in second group.
```{r stalk color below ring variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(stalk_color_below_ring) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(stalk_color_below_ring,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(stalk_color_below_ring,fill=class))+
  geom_bar(position = "dodge")
```

\newpage
### Veil color
The first graph below shows level n, o and y are less than 1% and big portion is level w.

The second graph shows big portion level is not significant to class so this variable is not used in model development step.
```{r veil color variable,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(veil_color) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(veil_color,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(veil_color,fill=class))+
  geom_bar(position = "dodge")
```

### Ring number
The first graph below shows level n is less than 1% and big portion is level o.

The second graph shows big portion level is not significant to class so this variable is not used in model development step.
```{r ring_number,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(ring_number) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(ring_number,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(ring_number,fill=class))+
  geom_bar(position = "dodge")
```

\newpage
### Ring type
The first graph below shows level f and n are less than 1% and big portion are e, l and p level.

The second graph shows e, l and p level are significant to class because majority of e is poisonous, all of l is poisonous and large majority of p is edible so this variable will be in second group.
```{r ring_type,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(ring_type) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(ring_type,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(ring_type,fill=class))+
  geom_bar(position = "dodge")
```

### Spore print color
The first graph below shows level b,o,u and y are less than 1% and big portion are h, k, n and w level.

The second graph shows h, k, n and w level are significant to class because large majority of h and w are poisonous and large majority of k and n are edible so this variable will be in second group.
```{r spore print color,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(spore_print_color) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(spore_print_color,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(spore_print_color,fill=class))+
  geom_bar(position = "dodge")
```

\newpage
### Population
The first graph below shows no level is less than 1% and s, v and y level are big portion.

The second graph shows s, v and y level are significant to class because large majority of s is edible, large majority of v is poisonous and majority of y is edible so this variable will be in first group.
```{r population,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(population) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(population,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(population,fill=class))+
  geom_bar(position = "dodge")
```

### Habitat
The first graph below shows no level is less than 1% and d, g and p level are big portion.

The second graph shows d, g and p level are significant to class because majority of d and g are edible and large majority of p is poisonous so this variable will be in first group.
```{r habitat,fig.show="hold",out.width="50%",echo=FALSE}
mushroom %>% group_by(habitat) %>% summarize(count=n()) %>%
  mutate(Percent=count/sum(count)) %>% 
  ggplot(aes(habitat,Percent)) +
  geom_bar(stat="identity")+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

mushroom %>% ggplot(aes(habitat,fill=class))+
  geom_bar(position = "dodge")
```

From all exploration above, 

First group variables are bruises, gill spacing, gill size, stalk shape, population and habitat.

Second group variables are bruises, gill spacing, gill size, stalk shape, population, habitat, cap surface, cap color, odor, gill color, stalk surface above ring,  stalk surface below ring, stalk color above ring, stalk color below ring, ring type and spore print color.

\newpage
## Model development
### Split Train and Test set
Mushroom dataset is split to train set and test set with portion 0.8 and 0.2 respectively. Train set is used for prediction model development and test set is used for performance validation.

```{r split, warning=FALSE}
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(mushroom$class,times = 1,p=0.2,list=FALSE)
train_set <- mushroom[-test_index,]
test_set <- mushroom[test_index,]
```
### Model
#### GLM  

Let's start with a simple model, generalized linear model(GLM) with first group variables.
```{r glm, warning=FALSE}
set.seed(1, sample.kind = "Rounding")
train_set_glm1 <- train_set %>% select(class,bruises,gill_spacing,gill_size,stalk_shape,
                                       population,habitat)
glm1 <- train(class ~ ., method = "glm", data = train_set_glm1)
predict_glm1 <- predict(glm1,test_set)
confusionMatrix(predict_glm1,test_set$class)
```

```{r glm result, echo=FALSE}
glm1_acc <- confusionMatrix(predict_glm1,test_set$class)$overall["Accuracy"]
glm1_sen <- confusionMatrix(predict_glm1,test_set$class)$byClass["Sensitivity"]
glm1_speci <- confusionMatrix(predict_glm1,test_set$class)$byClass["Specificity"]

result <- data.frame(Method="GLM(6 features)",Accuracy=glm1_acc,Sensitivity=glm1_sen,Specificity=glm1_speci)
rownames(result) <- NULL
knitr::kable(result)
```

Table above shows performance of this GLM model from confusion matrix. This model performance is quite good with 93.8% accuracy, 96% sensitivity and 91.6% specificity. Next, random forest method will be applied. Then, we will see if the performance is better compare to GLM.

#### Random forest  

Random forest method with first group variables will be applied and Then, compare performance with GLM method.
```{r rf1,warning=FALSE}
set.seed(1, sample.kind = "Rounding")
train_set_rf1 <- train_set %>% select(class,bruises,gill_spacing,gill_size,stalk_shape,
                                      population,habitat)
rf1 <- train(class~.,
             method="rf",
             data = train_set_rf1,
             tuneGrid = data.frame(mtry = 2))
predict_rf1 <- predict(rf1,test_set)
confusionMatrix(predict_rf1,test_set$class)
```

```{r rf1 result, echo=FALSE}
rf1_acc <- confusionMatrix(predict_rf1,test_set$class)$overall["Accuracy"]
rf1_sen <- confusionMatrix(predict_rf1,test_set$class)$byClass["Sensitivity"]
rf1_speci <- confusionMatrix(predict_rf1,test_set$class)$byClass["Specificity"]

result <- rbind(result,data.frame(Method="Random Forest(6 features)",
                                  Accuracy=rf1_acc,Sensitivity=rf1_sen,Specificity=rf1_speci))
rownames(result) <- NULL
knitr::kable(result)
```

The result of random forest method with first group variables  is 94.1% accuracy, 93.6% sensitivity and 94.6% specificity. Compare to previous GLM method, random forest accuracy and specificity are better but sensitivity is lower. From our goal of this project to maximize poisonous
mushroom prediction accuracy means maximize specificity. This random forest model is better than GLM method.

Next, I tuned mtry which is tuning parameter of random forest method from 1 to 3 to get the best performance of this method with first group variables.

```{r rf1 tuning, fig.align = 'center',out.width="70%",warning=FALSE}
set.seed(1, sample.kind = "Rounding")
rf1_cv <- train(class~.,
                method="rf",
                data = train_set_rf1,
                tuneGrid = data.frame(mtry = seq(1,3,1)))
plot(rf1_cv)
```

This plot shows result of mtry tuning from 1 to 3. The best performance comes from mtry = 3 with about 96% accuracy. Full result of the model is in Appendix B. Next, validate the model performance with test set.

```{r rf1 predicted}
predict_rf1_cv <- predict(rf1_cv,test_set)
confusionMatrix(predict_rf1_cv,test_set$class)
```

```{r rf1 tuning result,echo=FALSE}
rf1_cv_acc <- confusionMatrix(predict_rf1_cv,test_set$class)$overall["Accuracy"] 
rf1_cv_sen <- confusionMatrix(predict_rf1_cv,test_set$class)$byClass["Sensitivity"]
rf1_cv_speci <- confusionMatrix(predict_rf1_cv,test_set$class)$byClass["Specificity"]

result <- rbind(result,data.frame(Method="Random Forest with mtry tuning(6 features)",
                                            Accuracy=rf1_cv_acc,Sensitivity=rf1_cv_sen,Specificity=rf1_cv_speci))
rownames(result) <- NULL
knitr::kable(result)
```

The result of random forest method and mtry tuning with first group variables is 96% accuracy, 95.5% sensitivity and 96.6% specificity. Compare to previous two methods, random forest with mtry tuning accuracy and specificity are better but sensitivity is a little bit lower than GLM method. From our goal of this project to maximize specificity. This random forest model with mtry tuning is the best for now.

From the exploration data analysis section, there is second group variables. Let's apply these variables with random forest method with mtry tuning.

```{r rf2,fig.align = 'center',out.width="70%",warning=FALSE}
set.seed(1, sample.kind = "Rounding")
train_set_rf2 <- train_set %>% select(class,bruises,gill_spacing,gill_size,stalk_shape,
                                      population,habitat, cap_surface,cap_color,  
                                      odor,gill_color,stalk_surface_above_ring,
                                      stalk_surface_below_ring, stalk_color_above_ring,    
                                      stalk_color_below_ring,ring_type,spore_print_color)
rf2_cv <- train(class~.,
             method="rf",
             data = train_set_rf2,
             tuneGrid = data.frame(mtry = seq(1,4,1)))
plot(rf2_cv)
```

This plot shows result of mtry tuning from 1 to 4. The best performance comes from mtry = 4 with about 100% accuracy. Full result of the model is in Appendix B.

This model gave us the best performance but this model may lead to overfitting because we including variable which have less portion level(<1%). Top 20 variable importance is used to validate and adjust the model by removing the variable which is not in this top 20 and variable level is small portion(<5%).

```{r rf2 varimp}
varImp(rf2_cv)
```

From variable importance, variables which are in these top 20 are bruises, gill spacing, gill size, stalk shape, population, habitat, odor, stalk surface above ring, stalk surface below ring, stalk color below ring, 
ring type and spore print color but odor variable will be removed because odorp portion is less than 5%.

Let's try to fit model again with this adjusted variables.

```{r rf3,fig.align = 'center',out.width="70%",warning=FALSE}
set.seed(1, sample.kind = "Rounding")
train_set_rf3 <- train_set %>% select(class,bruises,gill_spacing,gill_size,stalk_shape,
                                      population,habitat,stalk_surface_above_ring,
                                      stalk_surface_below_ring,stalk_color_below_ring,
                                      ring_type,spore_print_color)
rf3_cv <- train(class~.,
                method="rf",
                data = train_set_rf3,
                tuneGrid = data.frame(mtry = seq(1,4,1)))
plot(rf3_cv)
```

This plot show result of mtry tuning from 1 to 4. The best performance comes from mtry = 4 with about 99.4% accuracy.
Full result of the model is in Appendix B.

```{r rf3 varimp}
varImp(rf3_cv)$importance %>% arrange(desc(Overall)) %>% head(10)
```

Let's check variable importance again. This time I checked only top 10 because less variables applied to the model.
From top 10 variable importance, there is no small portion level(<5%) so this model is acceptable. Then, apply this model to test set and see the performance.

```{r rf3 predicted}
predict_rf3_cv<- predict(rf3_cv,test_set)
confusionMatrix(predict_rf3_cv,test_set$class)
```

```{r rf3 result,echo=FALSE}
rf3_cv_acc <- confusionMatrix(predict_rf3_cv,test_set$class)$overall["Accuracy"] 
rf3_cv_sen <- confusionMatrix(predict_rf3_cv,test_set$class)$byClass["Sensitivity"]
rf3_cv_speci <- confusionMatrix(predict_rf3_cv,test_set$class)$byClass["Specificity"]

result <- rbind(result,data.frame(Method="Random Forest with mtry tuning(11 features)",
                                  Accuracy=rf3_cv_acc,Sensitivity=rf3_cv_sen,Specificity=rf3_cv_speci))
rownames(result) <- NULL
knitr::kable(result)
```

The result of random forest method and mtry tuning with adjusted second group variables is 99.2% accuracy, 98.5% sensitivity and 100% specificity. From the result, this is the best model. Next, XgBoost method will be applied and we will see it performance compare to previous methods.

\newpage
#### XgBoost  

Let's start applied XgBoost model with first group variables and compare the result with GLM and random forest.

```{r xg1,warning=FALSE}
set.seed(1, sample.kind = "Rounding")
train_set_xg1 <- train_set %>% 
  select(class,bruises,gill_spacing,gill_size,stalk_shape,population,habitat)
xg1 <- train(class~.,
             method="xgbTree",
             data = train_set_xg1)
```

The final values used for the model were nrounds = 50, max_depth = 3, eta = 0.4, gamma = 0,
 colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.75. Full result of the model is in Appendix B. Next, validate the model performance with test set.
 
```{r xg1 predicted}
predict_xg1 <- predict(xg1,test_set)
confusionMatrix(predict_xg1,test_set$class)
```

```{r xg1 result,echo=FALSE}
xg1_acc <- confusionMatrix(predict_xg1,test_set$class)$overall["Accuracy"] 
xg1_sen <- confusionMatrix(predict_xg1,test_set$class)$byClass["Sensitivity"]
xg1_speci <- confusionMatrix(predict_xg1,test_set$class)$byClass["Specificity"]

result <- rbind(result,data.frame(Method="XgBoost(6 features)",
                                  Accuracy=xg1_acc,Sensitivity=xg1_sen,Specificity=xg1_speci))
rownames(result) <- NULL
knitr::kable(result)
```

The result of XgBoost method with first group variables is 99.5% accuracy, 99.0% sensitivity and 100 % specificity. Compare to previous GLM method and random forest, XgBoost performance is better for all accuracy, sensitivity and specificity.

Next, second group variable will be applied.

```{r xg2,warning=FALSE}
set.seed(1, sample.kind = "Rounding")
train_set_xg2 <- train_set %>% select(class,bruises,gill_spacing,gill_size,stalk_shape,
                                      population,habitat,cap_surface,cap_color,odor,
                                      gill_color,stalk_surface_above_ring,stalk_surface_below_ring,
                                      stalk_color_above_ring,stalk_color_below_ring,
                                      ring_type,spore_print_color)
xg2 <- train(class~.,
             method="xgbTree",
             data = train_set_xg2)
```

This model gave us 100% accuracy with the final values used for the model were nrounds = 50, max_depth = 3, eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 1 which you can see full result of the model in Appendix B, but this model may lead to overfitting because we including variable which have less portion level(<1%). Top 20 variable importance is used to validate and adjust the model by removing the variable which is not in this top 20 and variable level is small portion(<5%).

```{r xg2 varimp}
varImp(xg2)
```

From variable importance, variables which are in these top 20 are bruises, gill spacing, gill size, population, habitat,stalk shape, cap color, odor, stalk surface above ring, stalk surface below ring, spore print color and ring type but odor, spore print color, population, habitat and stalk surface below ring will be removed because some level portion of these variables in top 20 is less than 5% such as odorp and odorc.

Let's try to fit model again with this adjusted variables.

```{r xg3,warning=FALSE}
set.seed(1, sample.kind = "Rounding")
train_set_xg3 <- train_set %>% select(class,bruises,gill_spacing,gill_size,stalk_shape,
                                      cap_color,stalk_surface_above_ring,ring_type)
xg3 <- train(class~.,
             method="xgbTree",
             data = train_set_xg3)
```

The final values used for the model were nrounds = 100, max_depth = 3, eta = 0.4, gamma = 0,
 colsample_bytree = 0.6, min_child_weight = 1 and subsample = 0.75. Full result of the model is in Appendix B. Next, validate the model performance with test set.

```{r xg3 predicted}
predict_xg3 <- predict(xg3,test_set)
confusionMatrix(predict_xg3,test_set$class)
```

```{r xg3 result,echo=FALSE}
xg3_acc <- confusionMatrix(predict_xg3,test_set$class)$overall["Accuracy"] 
xg3_sen <- confusionMatrix(predict_xg3,test_set$class)$byClass["Sensitivity"]
xg3_speci <- confusionMatrix(predict_xg3,test_set$class)$byClass["Specificity"]

result <- rbind(result,data.frame(Method="XgBoost(7 features)",
                                  Accuracy=xg3_acc,Sensitivity=xg3_sen,Specificity=xg3_speci))
rownames(result) <- NULL
knitr::kable(result)
```

The result of XgBoost method with adjusted second group variables is 99.1% accuracy, 99.9% sensitivity and 98.2% specificity. You can see this model provide the highest sensitivity but our goal is to maximize specificity so this model is not the best.

# Result
```{r result,echo=FALSE}
knitr::kable(result)
```

From the goal of this project, to develop model for mushroom class prediction which maximize poisonous mushroom prediction accuracy means maximize specificity, Random Forest with mtry tuning(11 features) and XgBoost(6 features) gave the best result with 100% specificity but XgBoost(6 features) have higher accuracy and sensitivity so the best model is XgBoost(6 features).

# Conclusion

From the result, random forest model performance improve when number of features are increased. When adding features which some levels are significant to mushroom class even though others level have less significant or less proportion but by building many trees and applied voting concept results performance improvement. 

XgBoost is opposite compared to random forest. While increasing features, its performance doesn't improve. By boosting concept when you add feature which some levels are significant to mushroom class but others level have less significant, this less significant affect the model performance. In the other hand, using only features which significant to mushroom class and have lower noise; in this case from less significant levels results better model performance. So the final model to predict mushroom class is XgBoost with 6 features(First group variables); bruises, gill spacing, gill size, stalk shape, population and habitat.

In this mushroom dataset, there are many variables which have significant levels to mushroom class but others level are less portion such as ring type so to minimize overfitting problem, these variables are not used in the model. In the future, if this dataset is updated, it is challenging to revisit and improve the model to have better performance. 

\newpage
# Appendix A
## Definition of each variable in mushroom dataset 

Class : e=edible, p=poisonous

Cap shape : b=bell, c=conical, x=convex, f=flat, k=knobbed, s=sunken

Cap surface : f=fibrous, g=grooves, y=scaly, s=smooth

Cap color : n=brown, b=buff, c=cinnamon, g=gray, r=green, p=pink, u=purple, e=red, w=white, y=yellow

Bruises : t=bruises, f=no

Odor : a=almond, l=anise, c=creosote, y=fishy, f=foul, m=musty, n=none, p=pungent, s=spicy

Gill attachment : a=attached, f=free

Gill spacing : c=close, w=crowded

Gill size : b=broad, n=narrow

Gill color : k=blank, n=brown, b=buff, h=chocolate, g=gray, r=green, o=orange, p=pink, u=purple, e=red, w=white, y=yellow

Stalk shape : e=enlarging, t=tapering

Stalk root : b=bulbous, c=club, u=cup, e=equal, z=rhizomorphs, r=rooted, ?=missing

Stalk surface above ring : f=fibrous, y=scaly, k=silky, s=smooth

Stalk surface below ring : f=fibrous, y=scaly, k=silky, s=smooth

Stalk color above ring : n=brown, b=buff, c=cinnamon, g=gray, o=orange, p=pink, e=red, w=white, y=yellow

Stalk color below ring : n=brown, b=buff, c=cinnamon, g=gray, o=orange, p=pink, e=red, w=white, y=yellow

Veil type : p=partial, u=universal

Veil color : n=brown, o=orange, w=white, y=yellow

Ring number : n=none, o=one, t=two

Ring type : c=cobwebby, e=evanescent, f=flaring, l=large, n=none, p=pendent, s=sheathing, z=zone

Spore print color : k=black, n=brown, b=buff, h=chocolate, r=green, o=orange, u=purple, w=white, y=yellow

Population : a=abundant, c=clustered, n=numerous, s=scattered, v=several, y=solitary

Habitat : g=grasses, l=leaves, m=meadows, paths=p, u=urban, w=waste, d=woods

\newpage
# Appendix B
## Fit model result
### GLM(6 features)

```{r glm fit}
glm1
```

### Random Forest(6 features) 
```{r rf1 fit}
rf1
```

### Random Forest with mtry tuning(6 features)
```{r rf1_cv fit}
rf1_cv
```

### Random Forest with mtry tuning(Second group variables)
```{r rf2_cv fit}
rf2_cv
```

### Random Forest with mtry tuning(11 features)
```{r rf3_cv fit}
rf3_cv
```

### XgBoost(6 features)
```{r xg1 fit}
xg1
```

### XgBoost(Second group variables)
```{r xg2 fit}
xg2
```

### XgBoost(7 features) 
```{r xg3 fit}
xg3
```
